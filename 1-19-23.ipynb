{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from pyzotero import zotero\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def write(x,y):\n",
    "    with open(x,'a') as f:\n",
    "        f.write(y)\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "with open('archive2022/zotero_keys.txt') as f:\n",
    "    for idx,x in enumerate(f):\n",
    "        y = x.split(' ')\n",
    "        z.append(y[-1].strip())\n",
    "        # print(idx,y)\n",
    "        if idx == 2:\n",
    "            print('\\n')\n",
    "            break\n",
    "\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "### use this to configure with your own api key and groupnum.\n",
    "\n",
    "###  https://pyzotero.readthedocs.io/en/latest/#getting-started-short-version\n",
    "\n",
    "group_id = z[1]\n",
    "api_key = z[2][1:-1]\n",
    "\n",
    "zot = zotero.Zotero(group_id,'group',api_key)\n",
    "cols = zot.collections()\n",
    "\n",
    "# itz = zot.everything(zot.collection_items(cols[-1]['key'])) \n",
    "#not helpful here, includes multiple entries of same\n",
    "\n",
    "itz1 = zot.everything(zot.collection_items_top(cols[-1]['key']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itz1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "for idx,_ in enumerate(range(len(itz1))):\n",
    "    x = itz1[idx]['data']['title']\n",
    "    y = itz1[idx]['data']['abstractNote']\n",
    "    z = [i['tag'] for i in itz1[idx]['data']['tags']]\n",
    "    A.append([x,y,z])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flow recommendations for the tributaries of the Great Lakes in New York and Pennsylvania',\n",
       " 'In order to address issues of flow management at a regional scale, this project has focused on defining and quantifying the ecological processes necessary to maintain intact aquatic ecosystems in streams ranging from small headwaters watersheds to large rivers. We have adapted the ELOHA framework in pursuit of recommendations for statewide management of river and stream flows that avoid “cumulative adverse impacts” (in the words of the Great Lakes Compact) to water-dependent natural resources.\\nWe began by identifying six guilds of fish and four guilds of mussels and aquatic insects, comprising a total of 43 species, whose fluvial dependencies are clear and well-documented. We selected these species as target organisms to represent all aquatic organisms and communities. Next, we defined components of the flow regime that would facilitate discussion of the flow needs of the target organisms and of the management steps necessary to meet these needs. We have been guided in the identification of these flow components – which include various degrees of high, seasonal, and low flows, and the statistics that define them – by considerable previous work, including flow recommendations for the Susquehanna Basin (Dephilip and Moberg 2011), the Ohio River watershed in Pennsylvania (DePhilip and Moberg 2013), and the basic discussion of flow components by Mathews and Richter (2007).\\nDocumentation of the associations between the critical life stages of the target organisms and particular flow components (sections 4 and 5, and Appendix 2) enabled the Technical Advisory Team to frame hypotheses of how these organisms (and the wider aquatic ecosystems they represent) will respond to altered flow regimes. Framing and evaluating these hypotheses enabled us to match the reproductive success of target organisms with variations in flow components and to provide the scientific basis for the flow recommendations in the preceding section.\\nTo evaluate the hypotheses of response to flow alteration, we employed causal-criteria analysis of the scientific literature, a structured approach for assessing the support a given study provides to a particular hypothesis. The use of this method, in place of quantitative testing of hypotheses, is an interim step made necessary by the lack of region-wide hydrologic simulations of unaltered flows and by limited streamflow gage data. We grouped hypotheses that address similar reproductive phases or flow components into generalized flow needs so that our flow recommendations efficiently address ecological processes that affect many different species simultaneously.\\nThese steps, guided by the Technical Advisory Team and the experience of similar projects, have led to the flow recommendations presented in Section 6. These recommendations address a dual need – to safeguard low flows, particularly during the drier periods of the year, and to preserve the elements of hydrologic variability above these low flows that are essential for reproduction and other life cycles needs of the diverse array of organisms that characterize aquatic ecosystems. To address the dual need, we recommend the use of two policy tools: passby flows for protection of low flows, and withdrawal limits to preserve the essential elements of hydrologic variability. The analyses in section 6 demonstrate that each tool is necessary but not sufficient by itself to preserve flow-dependent natural resources. We have taken the further step of testing the impact of these recommendations on a range of typical withdrawals in four streams representing the four stream size classes. The two policy tools acting in combination result in flow regimes that meet our flow recommendations, and thereby meet the documented flow needs of aquatic ecosystems.\\nThis project has assembled considerable information about the life cycles of the target fish, mussels, and aquatic macroinvertebrates. The information provided in Section 4 and Appendix 2, when combined with equivalent documentation assembled in the Susquehanna and Ohio River projects, constitutes a useful database for other states or large watersheds in the Northeastern U.S. or Great Lakes Basin, since many of the species are common to both of these regions.\\nThis project has shared with the Ohio River study (DePhilip and Moberg 2013) the same methodology to frame and evaluate hypotheses of response to flow alteration and to aggregate these hypotheses into flow needs. These flow needs, and the documented evaluation of their constituent hypotheses, provide a framework with broad applicability for flow management in the Northeast and Great Lakes.\\nWe have attempted in this report to frame flow recommendations that are science-based and also practical to apply in an actual water regulatory program. The flow recommendations employ hydrologic statistics that can readily be computed for each stream or stream reach, given information on simulated unaltered flows and current water withdrawals. At the moment, such information is only available for the Pennsylvanian portion of the project study area. Full application of the recommendations, within the Great Lakes and throughout New York State, will be achievable with the Streamflow Estimator Tool (SET), now nearing completion by the USGS Water Sciences Center in Troy. The SET will simulate the unaltered hydrograph for each stream in New York State, providing agency staff with desktop access to the flow management statistics used in this report. We believe this new tool, and the recommendations in this report, provide the basis for a water regulatory program that is science-based, transparent, and protective of aquatic ecosystems.',\n",
       " []]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.asarray(A,dtype=object)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assessment of environmental flow scenarios using state-and-transition models'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[27,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/albert/nltk_data'\n    - '/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m s5 \u001b[39m=\u001b[39m s4\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m s6 \u001b[39m=\u001b[39m s5\u001b[39m.\u001b[39mlower()\n\u001b[0;32m----> 9\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(s6)\n\u001b[1;32m     10\u001b[0m every \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39meverygrams(tokens,\u001b[39m2\u001b[39m,\u001b[39m4\u001b[39m)\n\u001b[1;32m     11\u001b[0m count0 \u001b[39m=\u001b[39m count0 \u001b[39m+\u001b[39m (collections\u001b[39m.\u001b[39mCounter(every))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/albert/nltk_data'\n    - '/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "count0 = collections.Counter()\n",
    "for idx,i in enumerate(A[:,0]):\n",
    "    s1 = i.replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\",'')\n",
    "    s4 = s3.replace(\",\",'')\n",
    "    s5 = s4.replace(\":\",'')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens,2,4)\n",
    "    count0 = count0 + (collections.Counter(every))\n",
    "count0 = count0.most_common()\n",
    "\n",
    "count1 = collections.Counter()\n",
    "for idx, i in enumerate(A[:,1]):\n",
    "    s1 = i.replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\",'')\n",
    "    s4 = s3.replace(\",\",'')\n",
    "    s5 = s4.replace(\":\",'')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens,2,4)\n",
    "    count1 = count1 + (collections.Counter(every))\n",
    "count1 = count1.most_common()\n",
    "\n",
    "count2 = collections.Counter()\n",
    "for idx, i in enumerate(A[:,2]):\n",
    "    x = collections.Counter([l.lower() for l in i])\n",
    "    count2 += x\n",
    "count2 = count2.most_common()\n",
    "\n",
    "\n",
    "\n",
    "count3 = count0 + count1 + count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
